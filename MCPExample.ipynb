{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrOUDyA2Cvhz62c1JOicSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhostScientist/Colab/blob/main/MCPExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Context Protocol (MCP) Demonstration\n",
        "\n",
        "This notebook demonstrates the core concepts of the **Model Context Protocol (MCP)**, an emerging standard for AI model interactions that promises to do for AI what REST did for web services.\n",
        "\n",
        "## What is MCP?\n",
        "\n",
        "MCP creates a standardized interface between applications and AI models, abstracting away provider-specific implementations. This allows developers to:\n",
        "\n",
        "- Write code that works with multiple AI providers without modification\n",
        "- Switch between models based on cost, capabilities, or performance\n",
        "- Transfer conversation context seamlessly between different models\n",
        "- Future-proof applications against changes in provider APIs\n",
        "\n",
        "Think of MCP as a \"universal adapter\" for AI models - write once, run everywhere. Need a new model? Build the model-specific plug, then play!"
      ],
      "metadata": {
        "id": "s2Vyg8QCDxMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2i_2luY9Bt9"
      },
      "outputs": [],
      "source": [
        "# MCP Demonstration: Standardizing AI Model Interactions\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Install required packages\n",
        "!pip install openai anthropic cohere -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from anthropic import Anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Problem MCP Solves\n",
        "\n",
        "Before diving into our MCP implementation, let's consider the current state of AI integration:\n",
        "\n",
        "Each AI provider has their own unique API structure:\n",
        "- Different parameter names (`messages` vs `prompt`)\n",
        "- Different authentication methods\n",
        "- Different response formats\n",
        "- Different context handling mechanisms\n",
        "- Different nomenclature for roles (`user` vs `human`)\n",
        "\n",
        "This fragmentation creates vendor lock-in, increases development time, and makes it difficult to benchmark or switch providers. The cells below demonstrate a solution to this problem through MCP."
      ],
      "metadata": {
        "id": "aWjvCgI1D2pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your API keys securely using environment variables\n",
        "# These will only exist for this Colab session - nothing is stored. If you want further control, feel free to make a copy of this notebook for yourself.\n",
        "\n",
        "# OpenAI API key\n",
        "openai_api_key = input(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Anthropic API key (optional)\n",
        "anthropic_api_key = input(\"Enter your Anthropic API key (or press Enter to skip): \")\n",
        "if anthropic_api_key:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXp0QH7-9RZb",
        "outputId": "fd255785-33d4-4c8c-9f60-6005e5f0cb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: \n",
            "Enter your Anthropic API key (or press Enter to skip): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCP Core Concepts\n",
        "\n",
        "The foundation of for the protocol is a standardized interface that abstracts away provider-specific details. Our example implementation focuses on three key principles:\n",
        "\n",
        "1. **Unified Message Format**: A standard structure for messages regardless of provider\n",
        "2. **Context Management**: Consistent handling of conversation history\n",
        "3. **Provider Abstraction**: A common interface that works across multiple AI models\n",
        "\n",
        "Our `MCPClient` base class defines this interface, which provider-specific implementations will adapt to their respective APIs."
      ],
      "metadata": {
        "id": "sGvpUMpuD7rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCPClient:\n",
        "    def __init__(self, provider_name: str = \"unknown\"):\n",
        "        self.provider_name = provider_name\n",
        "        self.context = []\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        standard_role = role.lower()\n",
        "        if standard_role not in [\"system\", \"user\", \"assistant\"]:\n",
        "            raise ValueError(f\"Invalid role: {role}. Must be 'system', 'user', or 'assistant'\")\n",
        "\n",
        "        self.context.append({\"role\": standard_role, \"content\": content})\n",
        "\n",
        "    def generate_response(self, prompt: Optional[str] = None,\n",
        "                          system: Optional[str] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate a response using the current context\n",
        "\n",
        "        Args:\n",
        "            prompt: Optional new user message to add before generating a response\n",
        "            system: Optional system message to set or update\n",
        "\n",
        "        Returns:\n",
        "            The AI model's response text\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def get_context(self) -> List[Dict[str, str]]:\n",
        "        return self.context.copy()\n",
        "\n",
        "    def set_context(self, context: List[Dict[str, str]]) -> None:\n",
        "        self.context = context.copy()\n",
        "\n",
        "    def clear_context(self) -> None:\n",
        "        self.context = []"
      ],
      "metadata": {
        "id": "12guG2AB9Xvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Provider-Specific Adapters\n",
        "\n",
        "With our standard interface defined, we now need to create \"adapters\" (those model-specific plugs I mentioned before!) for each AI provider. These adapters:\n",
        "\n",
        "- Translate between our standard MCP format and provider-specific APIs\n",
        "- Handle provider-specific requirements (like different parameter names)\n",
        "- Manage differences in context handling between providers\n",
        "\n",
        "This abstraction of integration is the value proposition of MCP. This pattern is similar to how database ORMs abstract away differences between database engines, allowing your application code to remain unchanged regardless of the underlying database."
      ],
      "metadata": {
        "id": "CYVfGnL9EAZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-Specific Considerations\n",
        "\n",
        "Even within the same provider, different models can have different requirements. For example, OpenAI's older models use a different parameter structure than their newer ones, and models like o1-mini don't support system messages.\n",
        "\n",
        "MCP should handle these differences transparently to the application. This is similar to how REST handles different API versions or capabilities without requiring client applications to change.\n",
        "\n",
        "The following example demonstrates how our MCP implementation handles model-specific differences:"
      ],
      "metadata": {
        "id": "c2G_8885EEwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenAIMCPClient(MCPClient):\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-4o-mini\"):\n",
        "        super().__init__(provider_name=\"openai\")\n",
        "        self.model = model\n",
        "\n",
        "        # Use provided API key or get from environment variable\n",
        "        if api_key:\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "        else:\n",
        "            self.client = OpenAI()\n",
        "\n",
        "    def generate_response(self, prompt: Optional[str] = None,\n",
        "                          system: Optional[str] = None) -> str:\n",
        "        # Example of sppecial handling for models that don't support system role\n",
        "        models_without_system_role = [\"o1-mini\"]\n",
        "        supports_system = self.model not in models_without_system_role\n",
        "\n",
        "        if system and supports_system:\n",
        "            has_system = any(msg[\"role\"] == \"system\" for msg in self.context)\n",
        "\n",
        "            if has_system:\n",
        "                for i, msg in enumerate(self.context):\n",
        "                    if msg[\"role\"] == \"system\":\n",
        "                        self.context[i] = {\"role\": \"system\", \"content\": system}\n",
        "                        break\n",
        "            else:\n",
        "                self.context.insert(0, {\"role\": \"system\", \"content\": system})\n",
        "        elif system and not supports_system:\n",
        "            prompt = f\"System instruction: {system}\\n\\nUser query: {prompt}\"\n",
        "\n",
        "        if prompt:\n",
        "            self.add_message(\"user\", prompt)\n",
        "\n",
        "        api_messages = self.context.copy()\n",
        "        if not supports_system:\n",
        "            api_messages = [msg for msg in api_messages if msg[\"role\"] != \"system\"]\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=api_messages\n",
        "            )\n",
        "\n",
        "            assistant_message = response.choices[0].message.content\n",
        "            self.add_message(\"assistant\", assistant_message)\n",
        "\n",
        "            return assistant_message\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "Uew_VhBh9bED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAIMCPClient(model=\"gpt-4o-mini\")\n",
        "\n",
        "response = openai_client.generate_response(\n",
        "    prompt=\"What is the Model Context Protocol?\",\n",
        "    system=\"You are a helpful AI assistant that explains technical concepts clearly and concisely.\"\n",
        ")\n",
        "\n",
        "print(f\"Response from {openai_client.provider_name} ({openai_client.model}):\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bow_HcpR9dSK",
        "outputId": "bd2c3207-5d04-4d86-bdbf-80d4e9871bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from openai (gpt-4o-mini):\n",
            "\n",
            "The Model Context Protocol (MCP) is a framework used primarily in artificial intelligence and machine learning to enhance the interactions between models and their environments. It provides a structured way for models to understand and adapt to the context in which they operate, allowing for more effective decision-making and action.\n",
            "\n",
            "Key components of the Model Context Protocol typically include:\n",
            "\n",
            "1. **Context Representation**: This refers to the information about the environment, constraints, goals, and other relevant factors that the model needs to consider. This could include data about user preferences, situational variables, or external conditions.\n",
            "\n",
            "2. **Adaptation Mechanism**: MCP includes strategies for models to adjust their behavior based on the context they are given. This might involve modifying parameters, switching between different algorithms, or changing the way information is processed.\n",
            "\n",
            "3. **Feedback Loop**: The protocol often incorporates mechanisms for feedback, allowing models to learn from the outcomes of their actions and to refine their understanding of the context over time.\n",
            "\n",
            "4. **Interoperability**: A key feature of MCP is its ability to facilitate communication and cooperation among multiple models or systems, enabling them to share contextual information and jointly solve problems.\n",
            "\n",
            "MCP is particularly relevant in areas like natural language processing, recommender systems, and robotics, where understanding the context can significantly influence performance and user satisfaction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnthropicMCPClient(MCPClient):\n",
        "    \"\"\"Anthropic-specific implementation of the MCP client\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"claude-3-5-sonnet-20241022\"):\n",
        "        super().__init__(provider_name=\"anthropic\")\n",
        "        self.model = model\n",
        "\n",
        "        # Use provided API key or get from environment variable\n",
        "        if api_key:\n",
        "            self.client = Anthropic(api_key=api_key)\n",
        "        else:\n",
        "            self.client = Anthropic()\n",
        "\n",
        "    def generate_response(self, prompt: Optional[str] = None,\n",
        "                          system: Optional[str] = None) -> str:\n",
        "        \"\"\"Generate a response using Anthropic's API\"\"\"\n",
        "\n",
        "        if prompt:\n",
        "            self.add_message(\"user\", prompt)\n",
        "\n",
        "        anthropic_messages = []\n",
        "\n",
        "        for msg in self.context:\n",
        "            if msg[\"role\"] != \"system\":\n",
        "                role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "                anthropic_messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
        "\n",
        "\n",
        "        if not anthropic_messages:\n",
        "            anthropic_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
        "\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=1024,\n",
        "                messages=anthropic_messages\n",
        "            )\n",
        "\n",
        "            has_system = any(msg[\"role\"] == \"system\" for msg in self.context)\n",
        "            if has_system or system:\n",
        "                print(\"Note: System message was ignored for Anthropic API call\")\n",
        "\n",
        "            assistant_message = response.content[0].text\n",
        "            self.add_message(\"assistant\", assistant_message)\n",
        "\n",
        "            return assistant_message\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "2TsYmA599koo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_anthropic_direct():\n",
        "    from anthropic import Anthropic\n",
        "\n",
        "    client = Anthropic()\n",
        "\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20241022\",\n",
        "            max_tokens=1024,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": \"Hello, please respond with one word: TESTING\"}\n",
        "            ]\n",
        "        )\n",
        "        print(\"API call successful!\")\n",
        "        print(f\"Response: {response.content[0].text}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"API call failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "test_result = test_anthropic_direct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjovdYO2_BTq",
        "outputId": "c094b167-7f3d-42b5-bcf2-edb65823e635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API call successful!\n",
            "Response: TESTING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Power of Provider Switching\n",
        "\n",
        "One of the major benefits of MCP is the ability to switch providers with minimal code changes. This enables:\n",
        "\n",
        "- A/B testing between different models\n",
        "- Fallback strategies when one provider is unavailable\n",
        "- Cost optimization by routing different tasks to different providers\n",
        "- Taking advantage of each provider's strengths\n",
        "\n",
        "The following sentiment analysis example demonstrates how the same code can work with different providers through our MCP abstraction:"
      ],
      "metadata": {
        "id": "uEjl3wg-ERxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text: str, client: MCPClient) -> str:\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of text using any MCP-compatible client\n",
        "\n",
        "    Args:\n",
        "        text: The text to analyze\n",
        "        client: Any MCP-compatible client\n",
        "\n",
        "    Returns:\n",
        "        The sentiment analysis result\n",
        "    \"\"\"\n",
        "\n",
        "    client.clear_context()\n",
        "\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a sentiment analysis assistant. Analyze the sentiment of the provided text\n",
        "    and respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\n",
        "    Provide no other text in your response.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    result = client.generate_response(\n",
        "        prompt=f\"Analyze the sentiment of this text: {text}\",\n",
        "        system=system_prompt\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "test_texts = [\n",
        "    \"I absolutely love this product! It's the best purchase I've made all year.\",\n",
        "    \"The service was terrible and the staff was rude. I'm never going back.\",\n",
        "    \"The movie was okay. It had some good moments but also some boring parts.\"\n",
        "]\n",
        "\n",
        "# Uncomment either OpenAI or Anthropic to test.\n",
        "\n",
        "# Test with OpenAI\n",
        "#print(\"SENTIMENT ANALYSIS WITH OPENAI:\\n\")\n",
        "#for text in test_texts:\n",
        "#    sentiment = analyze_sentiment(text, openai_client)\n",
        "#    print(f\"Text: {text}\")\n",
        "#    print(f\"Sentiment: {sentiment}\\n\")\n",
        "\n",
        "\n",
        "anthropic_client = AnthropicMCPClient()\n",
        "print(\"\\nSENTIMENT ANALYSIS WITH ANTHROPIC:\\n\")\n",
        "for text in test_texts:\n",
        "     sentiment = analyze_sentiment(text, anthropic_client)\n",
        "     print(f\"Text: {text}\")\n",
        "     print(f\"Sentiment: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BW2t_5s9lRi",
        "outputId": "8e8fdd4e-a3be-42ea-a9c4-baedd3300650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SENTIMENT ANALYSIS WITH ANTHROPIC:\n",
            "\n",
            "Note: System message was ignored for Anthropic API call\n",
            "Text: I absolutely love this product! It's the best purchase I've made all year.\n",
            "Sentiment: This text has a very positive sentiment. The use of enthusiastic language (\"absolutely love\"), superlatives (\"best\"), and exclamation points indicates strong positive emotions. The speaker expresses complete satisfaction with their purchase.\n",
            "\n",
            "Note: System message was ignored for Anthropic API call\n",
            "Text: The service was terrible and the staff was rude. I'm never going back.\n",
            "Sentiment: This text has a strongly negative sentiment. The use of words like \"terrible\" and \"rude\" express clear dissatisfaction, and the statement \"never going back\" reinforces the negative experience and shows complete rejection of the establishment.\n",
            "\n",
            "Note: System message was ignored for Anthropic API call\n",
            "Text: The movie was okay. It had some good moments but also some boring parts.\n",
            "Sentiment: This text expresses a mixed or neutral sentiment. The words \"okay\" and phrases like \"some good moments but also some boring parts\" indicate neither strong positive nor negative feelings, suggesting a balanced or lukewarm reaction to the movie.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Portability: A Game Changer\n",
        "\n",
        "Perhaps the most powerful feature of MCP is context portability - the ability to transfer conversation history between different providers. This enables:\n",
        "\n",
        "- Seamless handoffs between models with different specialties\n",
        "- Reducing costs by using more expensive models only when needed\n",
        "- Resilience against provider outages\n",
        "- Comparative analysis of model responses to the same context\n",
        "\n",
        "The following example demonstrates context transfer between providers:"
      ],
      "metadata": {
        "id": "5Zgr5pO1EUX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FYI: This example will only work if you have both API keys set up.\n",
        "\n",
        "def demonstrate_context_transfer():\n",
        "\n",
        "    openai_client = OpenAIMCPClient()\n",
        "    openai_client.clear_context()\n",
        "\n",
        "    print(\"Starting conversation with OpenAI:\\n\")\n",
        "\n",
        "    openai_client.generate_response(\n",
        "        system=\"You are a helpful assistant that provides concise information about planets.\",\n",
        "        prompt=\"Tell me about Mars.\"\n",
        "    )\n",
        "\n",
        "    print(f\"[OpenAI]: {openai_client.context[-1]['content']}\\n\")\n",
        "\n",
        "    openai_client.generate_response(prompt=\"What about its moons?\")\n",
        "    print(f\"[OpenAI]: {openai_client.context[-1]['content']}\\n\")\n",
        "\n",
        "    conversation_context = openai_client.get_context()\n",
        "\n",
        "    try:\n",
        "        anthropic_client = AnthropicMCPClient()\n",
        "\n",
        "        anthropic_client.set_context(conversation_context)\n",
        "\n",
        "        print(\"Continuing conversation with Anthropic:\\n\")\n",
        "        response = anthropic_client.generate_response(\n",
        "            prompt=\"How does gravity on Mars compare to Earth?\"\n",
        "        )\n",
        "\n",
        "        print(f\"[Anthropic]: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't test with Anthropic: {str(e)}\")\n",
        "        print(\"You may need to set up your Anthropic API key to run this example.\")\n",
        "\n",
        "\n",
        "demonstrate_context_transfer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp6e_oyz9ssh",
        "outputId": "a91bbba3-454a-4b19-a662-03c7c1c52b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversation with OpenAI:\n",
            "\n",
            "[OpenAI]: Mars is the fourth planet from the Sun in our solar system and is often called the \"Red Planet\" due to its reddish appearance, which is a result of iron oxide (rust) on its surface. Here are some key facts about Mars:\n",
            "\n",
            "1. **Size and Structure**: Mars has a diameter of about 6,779 kilometers (4,212 miles), making it about half the size of Earth. It has a thin atmosphere, composed mostly of carbon dioxide.\n",
            "\n",
            "2. **Moons**: Mars has two small moons, Phobos and Deimos, which are thought to be captured asteroids.\n",
            "\n",
            "3. **Surface Features**: The planet features the largest volcano in the solar system, Olympus Mons, and a massive canyon system, Valles Marineris. It also has polar ice caps made of water and carbon dioxide.\n",
            "\n",
            "4. **Water**: Mars has evidence of past water flows and currently has water ice beneath its surface, with seasonal dark streaks suggesting possible briny liquid water flows.\n",
            "\n",
            "5. **Exploration**: Mars has been explored by numerous spacecraft, including rovers like Curiosity and Perseverance, which are studying its geology and searching for signs of past life.\n",
            "\n",
            "6. **Conditions**: The surface temperature varies widely, averaging around -80 degrees Fahrenheit (-62 degrees Celsius). It experiences dust storms that can cover the entire planet.\n",
            "\n",
            "7. **Potential for Life**: Mars has long been a target for the search for extraterrestrial life due to its past conditions that may have been suitable for life.\n",
            "\n",
            "Mars continues to be a focus of scientific exploration, with future manned missions planned by several space agencies.\n",
            "\n",
            "[OpenAI]: Mars has two small moons, Phobos and Deimos. Here are some details about each:\n",
            "\n",
            "### Phobos\n",
            "1. **Size and Shape**: Phobos is the larger of Mars' two moons, with a diameter of about 22 kilometers (14 miles). It has a irregular shape, resembling a potato more than a sphere.\n",
            "\n",
            "2. **Orbit**: It orbits Mars very closely, at an average distance of about 6,000 kilometers (3,700 miles) above the surface. It takes just about 7 hours and 39 minutes to complete one orbit, which is shorter than a Martian day.\n",
            "\n",
            "3. **Surface Features**: The surface of Phobos is covered with craters, the largest being Stickney Crater. Its low gravity means that it has a very weak atmosphere and constantly shed material.\n",
            "\n",
            "4. **Fate**: Phobos is slowly spiraling inward towards Mars and is expected to either crash into the planet or break apart to form a ring system in about 50 million years.\n",
            "\n",
            "### Deimos\n",
            "1. **Size and Shape**: Deimos is smaller than Phobos, with a diameter of about 12 kilometers (7.5 miles). Like Phobos, it also has an irregular, potato-like shape.\n",
            "\n",
            "2. **Orbit**: Deimos orbits at a greater distance than Phobos, approximately 23,460 kilometers (14,580 miles) from the Martian surface. It takes about 30.3 hours to complete one orbit, which means it rises in the east and sets in the west.\n",
            "\n",
            "3. **Surface Features**: Deimos' surface is smoother than that of Phobos, covered with a layer of fine dust and regolith, giving it a less cratered appearance.\n",
            "\n",
            "4. **Origin**: Both moons are thought to be captured asteroids from the asteroid belt between Mars and Jupiter due to their composition and shape.\n",
            "\n",
            "### Overall\n",
            "Both moons are of significant interest to scientists studying the history and evolution of the Martian system. While they are small and irregularly shaped compared to Earth's moon, their proximity to Mars and unique characteristics make them valuable targets for future exploration.\n",
            "\n",
            "Continuing conversation with Anthropic:\n",
            "\n",
            "Note: System message was ignored for Anthropic API call\n",
            "[Anthropic]: Gravity on Mars is approximately 38% of Earth's gravity (or about one-third). Here's a detailed comparison:\n",
            "\n",
            "Earth's Gravity:\n",
            "- 9.81 meters per second squared (m/s²)\n",
            "- 1 g\n",
            "\n",
            "Mars' Gravity:\n",
            "- 3.72 meters per second squared (m/s²)\n",
            "- 0.38 g\n",
            "\n",
            "This means that:\n",
            "1. A person who weighs 100 pounds (45 kg) on Earth would weigh about 38 pounds (17 kg) on Mars\n",
            "2. Objects fall more slowly on Mars\n",
            "3. It's easier to jump higher and throw things farther on Mars\n",
            "4. The lower gravity affects how things work and behave:\n",
            "   - Fluid dynamics\n",
            "   - Plant growth\n",
            "   - Human physiology\n",
            "   - Construction and engineering\n",
            "\n",
            "The reduced gravity on Mars presents both advantages and challenges for potential human colonization:\n",
            "\n",
            "Advantages:\n",
            "- Easier to move heavy objects\n",
            "- Less structural support needed for buildings\n",
            "- Lower launch energy needed to leave Mars\n",
            "\n",
            "Challenges:\n",
            "- Human health issues (muscle atrophy, bone loss)\n",
            "- Different design requirements for equipment and vehicles\n",
            "- Adapting Earth-based technologies to work in lower gravity\n",
            "\n",
            "This difference in gravity is an important consideration in planning Mars missions and potential colonization efforts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Theory to Practice: MCP in Production\n",
        "\n",
        "This notebook has demonstrated a simplified implementation of MCP concepts. In a production environment, you might:\n",
        "\n",
        "1. Use a full-featured MCP library rather than building your own\n",
        "2. Add advanced features like:\n",
        "   - Streaming responses\n",
        "   - Tool use standardization\n",
        "   - Error handling normalization\n",
        "   - Prompt template management\n",
        "\n",
        "The key takeaway is that standardization through protocols like MCP creates significant advantages:\n",
        "\n",
        "- **Reduced development time**: Integrate once, use many providers\n",
        "- **Future-proofing**: Switch to better models as they emerge\n",
        "- **Negotiating power**: Avoid vendor lock-in\n",
        "- **Architectural flexibility**: Design multi-model systems easily\n",
        "\n",
        "As AI becomes increasingly central to applications, the need for standardization becomes more critical. MCP represents an important step toward a more mature, interoperable AI ecosystem."
      ],
      "metadata": {
        "id": "4_n0Lhu0EbrM"
      }
    }
  ]
}